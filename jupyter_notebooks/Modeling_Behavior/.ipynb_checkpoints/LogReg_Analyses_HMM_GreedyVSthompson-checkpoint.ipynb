{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "This logistic regression is based on the Beeler/Daw et al. 2010 paper.\n",
    "Specifically:\n",
    "\n",
    "dependent variable: binary choice of port (-1 or 1)\n",
    "\n",
    "explanatory variables: \n",
    "1. the N previous rewards $ r_{t-N:t-1} $\n",
    "2. the previous choice $c_{t-1}$ to capture a tendency to stay or switch\n",
    "3. bias variable (1) to capture fixed, overall preference for either port\n",
    "\n",
    "Note: this model only carries information about ports when it gets a reward. IE -1 = right reward, 1 = left reward, but 0 = no reward (for either side). Should compare models with this information vs. added information about the non-rewarded port choices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02/19/2017\n",
    "I just ran the above code on the following data:\n",
    "    \n",
    "- `full_9010_02192017.csv`\n",
    "- `full_8020_02192017.csv`\n",
    "- `full_7030_02192017.csv`\n",
    "\n",
    "Which were computed by compiling the following data:\n",
    "- block range: exactly 50 rewards\n",
    "- p(choose high Port) >= [p-0.1] where p = p(high Port)\n",
    "\n",
    "Observations:\n",
    "1. pseudo-R2 also similar: ~0.64-0.69\n",
    "2. For (1)  maybe a slight trend that 90-10 had a better model, but not certain. \n",
    "3. For each condition, I tried adding in more ports (up to 3) into the past to see if it would help out. For all 3 conditions, decisions for t-N where 2 >= 2 had coefficients = 0. That is, the only non-zero beta for the ports was the most recent one. \n",
    "\n",
    "Next step: \n",
    "\n",
    "\n",
    "**Figures**\n",
    "\n",
    "1. Logistic regression performs similarly across different conditions\n",
    "    - x axis: 90-10,80-20,70-30\n",
    "    - y axis: F1 score, pseudo-R2\n",
    "2. Comparing model flexibility across different conditions\n",
    "    - x axis: number of previous rewards included (i.e. parameters in model)\n",
    "    - y axis: BIC\n",
    "    - color: each condition\n",
    "3. Knowing where the non-rewarded trials are:\n",
    "    - what if you did the same regression, but instead you know where the non-rewarded trials are?\n",
    "    - also, compare to adding in previous ports (or rewards in the above scenario) and compare\n",
    "\n",
    "4. Comparing 'strategies' across conditions\n",
    "    - train on 90-10, test on 80-20 (and all combinations)\n",
    "\n",
    "5. Comparing 'strategies' across mice\n",
    "    - train on one mouse:condition pair, and test on another mouse:condition pair (for the same condition)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/shayneufeld/GitHub/mouse_bandit/data_preprocessing_code')\n",
    "sys.path.append('/Users/shayneufeld/GitHub/mouse_bandit')\n",
    "import support_functions as sf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import bandit_preprocessing as bp\n",
    "import sklearn.linear_model\n",
    "import sklearn.tree\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define function to do logistic regression and some basic evalutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "I compiled the code above into a more compact function so I can cycle through different conditions/mice/etc \n",
    "as neccessary\n",
    "'''\n",
    "\n",
    "def logreg_and_eval(data,num_rewards=10,test_data=False):\n",
    "    '''\n",
    "    Perform Logistic Regression on a pandas dataframe of trials (from feature matrix)\n",
    "    \n",
    "    Inputs:\n",
    "        - data: pandas dataframe of trials (from feature matrix)\n",
    "    Outputs:\n",
    "        - logreg: trained logistic regression model (from sklearn)\n",
    "        - stats:  pandas dataframe with F1, pseudo-R2, and BIC scores from model\n",
    "        - coeffs: beta coefficients from logreg\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    from statsmodels.discrete.discrete_model import Logit\n",
    "    \n",
    "    port_features = []\n",
    "    reward_features = []\n",
    "\n",
    "    #change right port to -1 instead of 0\n",
    "    for col in data:\n",
    "        if '_Port' in col:\n",
    "            data.loc[data[col] == 0,col] = -1\n",
    "            port_features.append(col)\n",
    "        elif '_Reward' in col:\n",
    "            reward_features.append(col)\n",
    "\n",
    "    #create new feature matrix\n",
    "    d = data.copy()\n",
    "    for i in range(len(port_features)):\n",
    "        d[reward_features[i]] = d[reward_features[i]].values*d[port_features[i]].values\n",
    "    \n",
    "    \n",
    "    #determine the features\n",
    "    features = reward_features.copy()\n",
    "    features = features[-1*num_rewards:] #only take the num of rewards specificied in the function\n",
    "    features.append('1_Port') #append the last decision as a feature\n",
    "    features.append('Decision') #finally append the decision so we can take it to predict later\n",
    "    \n",
    "    #final version of data\n",
    "    d = d[features].copy() #this now just has the features we want and the decision we want to predict\n",
    "    \n",
    "    #do the same thing for the test data if it exists!\n",
    "    if test_data is not False:\n",
    "        for col in test_data:\n",
    "            if '_Port' in col:\n",
    "                test_data.loc[test_data[col] == 0,col] = -1\n",
    "\n",
    "        #create new feature matrix\n",
    "        data_test_new = test_data.copy()\n",
    "        for i in range(len(port_features)):\n",
    "            data_test_new[reward_features[i]] = test_data[reward_features[i]].values*test_data[port_features[i]].values\n",
    "        \n",
    "        d_test = data_test_new[features].copy()\n",
    "    \n",
    "    \n",
    "        #set training and testing sets now\n",
    "        x_train = d.iloc[:,:-1].values\n",
    "        y_train = d.iloc[:,-1].values\n",
    "        x_test = d_test.iloc[:,:-1].values\n",
    "        y_test = d_test.iloc[:,-1].values\n",
    "        \n",
    "        prev_port_test = d_test['1_Port'].values\n",
    "        prev_port_test[prev_port_test==-1] = 0\n",
    "    \n",
    "    #if there is no test data, then split up the data into training and testing\n",
    "    else:\n",
    "        #extract features and decisions\n",
    "        x = d.iloc[:,:-1].values\n",
    "        y = d.iloc[:,-1].values\n",
    "\n",
    "        #split into training and testing\n",
    "        n_trials = x.shape[0]\n",
    "        shuf_inds = np.random.permutation(n_trials)\n",
    "        split_ind = int(n_trials*0.7)\n",
    "\n",
    "        x_train = x[shuf_inds[:split_ind],:]\n",
    "        y_train = y[shuf_inds[:split_ind]]\n",
    "\n",
    "        x_test = x[shuf_inds[split_ind:],:]\n",
    "        y_test = y[shuf_inds[split_ind:]]\n",
    "        \n",
    "        #extract previous port decision for test set\n",
    "        #these will be used to calculate switches on the test predictions\n",
    "        prev_port_test = d['1_Port'].values[shuf_inds[split_ind:]]\n",
    "        prev_port_test[prev_port_test==-1] = 0\n",
    "    \n",
    "    '''\n",
    "    Modeling\n",
    "    '''\n",
    "    \n",
    "    #fit logistic regression\n",
    "    logreg = sklearn.linear_model.LogisticRegressionCV()\n",
    "    logreg.fit(x_train,y_train)\n",
    "    \n",
    "    #predict on testing set\n",
    "    y_predict = logreg.predict(x_test)\n",
    "    y_predict_proba = logreg.predict_proba(x_test)\n",
    "    \n",
    "    #model accuracy\n",
    "    score = logreg.score(x_test,y_test)\n",
    "    \n",
    "    #calculating pseudo-R2 and BIC from statsmodel OLS\n",
    "    #model = Logit(y_train,x_train)\n",
    "    #rslt  = model.fit()\n",
    "\n",
    "    #switches\n",
    "    y_test_switch = np.abs(y_test - prev_port_test)\n",
    "    y_predict_switch = np.abs(y_predict - prev_port_test)\n",
    "    acc_pos,acc_neg,F1=sf.score_both_and_confuse(y_predict_switch,y_test_switch,confusion=False,disp=True)\n",
    "    \n",
    "    #extract coefficients\n",
    "    coefs = logreg.coef_ #retrieve coefs\n",
    "    coefs = np.append(coefs[0],logreg.intercept_) #add bias coef\n",
    "    \n",
    "    #create stats database to return\n",
    "    d_ = {'F1':F1,'Accuracy':score}\n",
    "    stats = pd.DataFrame(data=d_,index=[0])\n",
    "    features = features[:-1]\n",
    "    features.append('Bias')\n",
    "    \n",
    "    coefs = pd.DataFrame(data=coefs.reshape(1,-1),columns=features)\n",
    "    return logreg,stats,coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logreg_and_eval_withports(data,num_rewards=10,num_ports=1,test_data=False):\n",
    "    '''\n",
    "    Perform Logistic Regression on a pandas dataframe of trials (from feature matrix)\n",
    "    \n",
    "    Inputs:\n",
    "        - data: pandas dataframe of trials (from feature matrix)\n",
    "    Outputs:\n",
    "        - logreg: trained logistic regression model (from sklearn)\n",
    "        - stats:  pandas dataframe with F1, pseudo-R2, and BIC scores from model\n",
    "        - coeffs: beta coefficients from logreg\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    from statsmodels.discrete.discrete_model import Logit\n",
    "    \n",
    "    port_features = []\n",
    "    reward_features = []\n",
    "\n",
    "    #change right port to -1 instead of 0\n",
    "    for col in data:\n",
    "        if '_Port' in col:\n",
    "            data.loc[data[col] == 0,col] = -1\n",
    "            port_features.append(col)\n",
    "        elif '_Reward' in col:\n",
    "            reward_features.append(col)\n",
    "\n",
    "    #create new feature matrix\n",
    "    d = data.copy()\n",
    "    for i in range(len(port_features)):\n",
    "        d[reward_features[i]] = d[reward_features[i]].values*d[port_features[i]].values\n",
    "    \n",
    "    \n",
    "    #determine the features\n",
    "    features = reward_features.copy()\n",
    "    if num_rewards == 0:\n",
    "        features = port_features[-1*num_ports:]\n",
    "    elif num_ports == 0:\n",
    "        features = features[-1*num_rewards:] #only take the num of rewards specificied in the function\n",
    "    else:\n",
    "        features = features[-1*num_rewards:]\n",
    "        features = np.append(features,port_features[-1*num_ports:])\n",
    "    \n",
    "    print(features)\n",
    "    features = np.append(features,'Decision') #finally append the decision so we can take it to predict later\n",
    "    \n",
    "    \n",
    "    \n",
    "    #final version of data\n",
    "    d = d[features].copy() #this now just has the features we want and the decision we want to predict\n",
    "    \n",
    "    #do the same thing for the test data if it exists!\n",
    "    if test_data is not False:\n",
    "        for col in test_data:\n",
    "            if '_Port' in col:\n",
    "                test_data.loc[test_data[col] == 0,col] = -1\n",
    "\n",
    "        #create new feature matrix\n",
    "        data_test_new = test_data.copy()\n",
    "        for i in range(len(port_features)):\n",
    "            data_test_new[reward_features[i]] = test_data[reward_features[i]].values*test_data[port_features[i]].values\n",
    "        \n",
    "        d_test = data_test_new[features].copy()\n",
    "    \n",
    "    \n",
    "        #set training and testing sets now\n",
    "        x_train = d.iloc[:,:-1].values\n",
    "        y_train = d.iloc[:,-1].values\n",
    "        x_test = d_test.iloc[:,:-1].values\n",
    "        y_test = d_test.iloc[:,-1].values\n",
    "        \n",
    "        prev_port_test = test_data['1_Port'].values\n",
    "        prev_port_test[prev_port_test==-1] = 0\n",
    "    \n",
    "    #if there is no test data, then split up the data into training and testing\n",
    "    else:\n",
    "        #extract features and decisions\n",
    "        x = d.iloc[:,:-1].values\n",
    "        y = d.iloc[:,-1].values\n",
    "\n",
    "        #split into training and testing\n",
    "        n_trials = x.shape[0]\n",
    "        shuf_inds = np.random.permutation(n_trials)\n",
    "        split_ind = int(n_trials*0.7)\n",
    "\n",
    "        x_train = x[shuf_inds[:split_ind],:]\n",
    "        y_train = y[shuf_inds[:split_ind]]\n",
    "\n",
    "        x_test = x[shuf_inds[split_ind:],:]\n",
    "        y_test = y[shuf_inds[split_ind:]]\n",
    "        \n",
    "        #extract previous port decision for test set\n",
    "        #these will be used to calculate switches on the test predictions\n",
    "        prev_port_test = data['1_Port'].values[shuf_inds[split_ind:]]\n",
    "        prev_port_test[prev_port_test==-1] = 0\n",
    "    \n",
    "    '''\n",
    "    Modeling\n",
    "    '''\n",
    "    \n",
    "    #fit logistic regression\n",
    "    logreg = sklearn.linear_model.LogisticRegressionCV()\n",
    "    logreg.fit(x_train,y_train)\n",
    "    \n",
    "    #predict on testing set\n",
    "    y_predict = logreg.predict(x_test)\n",
    "    y_predict_proba = logreg.predict_proba(x_test)\n",
    "    \n",
    "    #model accuracy\n",
    "    score = logreg.score(x_test,y_test)\n",
    "    \n",
    "    #calculating pseudo-R2 and BIC from statsmodel OLS\n",
    "    #model = Logit(y_train,x_train)\n",
    "    #rslt  = model.fit()\n",
    "\n",
    "    #switches\n",
    "    y_test_switch = np.abs(y_test - prev_port_test)\n",
    "    y_predict_switch = np.abs(y_predict - prev_port_test)\n",
    "    acc_pos,acc_neg,F1=sf.score_both_and_confuse(y_predict_switch,y_test_switch,confusion=False,disp=True)\n",
    "    \n",
    "    #extract coefficients\n",
    "    coefs = logreg.coef_ #retrieve coefs\n",
    "    coefs = np.append(coefs[0],logreg.intercept_) #add bias coef\n",
    "    \n",
    "    #create stats database to return\n",
    "    d_ = {'F1':F1,'Accuracy':score}\n",
    "    stats = pd.DataFrame(data=d_,index=[0])\n",
    "    features = features[:-1]\n",
    "    features = np.append(features,'Bias')\n",
    "    \n",
    "    coefs = pd.DataFrame(data=coefs.reshape(1,-1),columns=features)\n",
    "    return logreg,stats,coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_t = pd.read_csv('/Users/shayneufeld/GitHub/mouse_bandit/data/processed_data/hmm_matrix_full_8020.csv',index_col=0)\n",
    "data_g = pd.read_csv('/Users/shayneufeld/GitHub/mouse_bandit/data/processed_data/hmm_matrix_full_8020_greedy.csv',index_col=0)\n",
    "\n",
    "datas = [data_t,data_g]\n",
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,d in enumerate(datas):\n",
    "    \n",
    "    for j in range(30):\n",
    "        model_curr,stats_curr,coefs_curr = logreg_and_eval(d,num_rewards=10)\n",
    "        models.append(models)\n",
    "\n",
    "        if ((i == 0 and j == 0)):\n",
    "            stats = stats_curr.copy()\n",
    "            coefs = coefs_curr.copy()\n",
    "        else:\n",
    "            stats = stats.append(stats_curr)\n",
    "            coefs = coefs.append(coefs_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shayneufeld/anaconda/envs/cagrin/lib/python3.5/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/shayneufeld/anaconda/envs/cagrin/lib/python3.5/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "stats.index = np.arange(stats.shape[0])\n",
    "coefs.index = np.arange(stats.shape[0])\n",
    "\n",
    "stats['Condition'] = 'Thompson'\n",
    "stats.iloc[30:]['Condition'] = 'Greedy'\n",
    "\n",
    "coefs['Condition'] = 'Thompson'\n",
    "coefs.iloc[30:]['Condition'] = 'Greedy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing beta coefficients across conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-b705b5807c21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m plt.errorbar(np.arange(coefs.shape[1]-1),coefs[coefs['Condition'] == 'Greedy'].iloc[:,1:].mean(),\n\u001b[1;32m      7\u001b[0m              \u001b[0myerr\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcoefs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoefs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Condition'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Greedy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoefs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Condition'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Greedy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m              color='red',alpha=0.5)\n\u001b[0m\u001b[1;32m      9\u001b[0m plt.errorbar(np.arange(coefs.shape[1]-1),coefs[coefs['Condition'] == 'Thompson'].iloc[:,1:].mean(),\n\u001b[1;32m     10\u001b[0m              \u001b[0myerr\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcoefs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoefs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Condition'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Thompson'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoefs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Condition'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Thompson'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shayneufeld/anaconda/envs/cagrin/lib/python3.5/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36merrorbar\u001b[0;34m(x, y, yerr, xerr, fmt, ecolor, elinewidth, capsize, barsabove, lolims, uplims, xlolims, xuplims, errorevery, capthick, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   2835\u001b[0m                           \u001b[0mxlolims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxlolims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxuplims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxuplims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2836\u001b[0m                           \u001b[0merrorevery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrorevery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapthick\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcapthick\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2837\u001b[0;31m                           **kwargs)\n\u001b[0m\u001b[1;32m   2838\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2839\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwashold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shayneufeld/anaconda/envs/cagrin/lib/python3.5/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1817\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1818\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1819\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1820\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shayneufeld/anaconda/envs/cagrin/lib/python3.5/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36merrorbar\u001b[0;34m(self, x, y, yerr, xerr, fmt, ecolor, elinewidth, capsize, barsabove, lolims, uplims, xlolims, xuplims, errorevery, capthick, **kwargs)\u001b[0m\n\u001b[1;32m   2929\u001b[0m             \u001b[0mnoylims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlolims\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0muplims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2930\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnoylims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2931\u001b[0;31m                 \u001b[0mxo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxywhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoylims\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0meverymask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2932\u001b[0m                 \u001b[0mlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxywhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoylims\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0meverymask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2933\u001b[0m                 \u001b[0mbarcols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mlines_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shayneufeld/anaconda/envs/cagrin/lib/python3.5/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mxywhere\u001b[0;34m(xs, ys, mask)\u001b[0m\n\u001b[1;32m   2817\u001b[0m             \u001b[0mys\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2818\u001b[0m             \"\"\"\n\u001b[0;32m-> 2819\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2820\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2821\u001b[0m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mthisx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthisx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAFoCAYAAABQVZB6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113295a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(12,4))\n",
    "gs = gridspec.GridSpec(1,4,width_ratios=[2,1,1,1])\n",
    "\n",
    "plt.subplot(gs[0])\n",
    "plt.errorbar(np.arange(coefs.shape[1]-1),coefs[coefs['Condition'] == 'Greedy'].iloc[:,1:].mean(),\n",
    "             yerr= coefs[coefs['Condition'] == 'Greedy'].iloc[:,1:].mean() / np.sqrt(np.sum(coefs['Condition'] == 'Greedy')),\n",
    "             color='red',alpha=0.5)\n",
    "plt.errorbar(np.arange(coefs.shape[1]-1),coefs[coefs['Condition'] == 'Thompson'].iloc[:,1:].mean(),\n",
    "             yerr= coefs[coefs['Condition'] == 'Thompson'].iloc[:,1:].mean() / np.sqrt(np.sum(coefs['Condition'] == 'Thompson')),\n",
    "             color='green',alpha=0.5)\n",
    "\n",
    "plt.scatter(np.arange(coefs.shape[1]-1),coefs[coefs['Condition'] == 'Greedy'].iloc[:,1:].mean(),\n",
    "            color='red',label='90-10',s=50)\n",
    "plt.scatter(np.arange(coefs.shape[1]-1),coefs[coefs['Condition'] == 'Thompson'].iloc[:,1:].mean(),\n",
    "            color='green',label='80-20',s=50)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.xticks(np.arange(coefs.shape[1]),coefs.columns.values[1:],rotation='vertical')\n",
    "plt.xlim(-0.5,8.5)\n",
    "plt.title('Regression Coefficients')\n",
    "\n",
    "plt.subplot(gs[1])\n",
    "sns.barplot(x='Condition',y='Accuracy',data=stats)\n",
    "plt.title('Accuracy (test)')\n",
    "plt.ylim(0.7,1)\n",
    "\n",
    "plt.subplot(gs[2])\n",
    "sns.barplot(x='Condition',y='F1',data=stats)\n",
    "plt.title('F1 (test)')\n",
    "#plt.ylim(0.7,1)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test number of parameters / model flexibility vs BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>BIC</th>\n",
       "      <th>F1</th>\n",
       "      <th>negative loglikelihood</th>\n",
       "      <th>pseudo-R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.861862</td>\n",
       "      <td>5225.774873</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>2577.476777</td>\n",
       "      <td>0.468250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.858859</td>\n",
       "      <td>5143.249004</td>\n",
       "      <td>0.567926</td>\n",
       "      <td>2536.213842</td>\n",
       "      <td>0.476742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Condition  Accuracy          BIC        F1  negative loglikelihood  \\\n",
       "0       70.0  0.861862  5225.774873  0.566038             2577.476777   \n",
       "0       70.0  0.858859  5143.249004  0.567926             2536.213842   \n",
       "\n",
       "   pseudo-R2  \n",
       "0   0.468250  \n",
       "0   0.476742  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.011477\n",
      "         Iterations: 35\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2853.0            0.0\n",
      "True YES           4.0          140.0\n",
      "\n",
      "F1: 0.986\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: 0.97\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shayneufeld/anaconda/envs/cagrin/lib/python3.5/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.009128\n",
      "         Iterations: 35\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2860.0            0.0\n",
      "True YES           5.0          132.0\n",
      "\n",
      "F1: 0.981\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: 0.96\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shayneufeld/anaconda/envs/cagrin/lib/python3.5/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.008785\n",
      "         Iterations: 35\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2862.0            0.0\n",
      "True YES           5.0          130.0\n",
      "\n",
      "F1: 0.981\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: 0.96\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shayneufeld/anaconda/envs/cagrin/lib/python3.5/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.012267\n",
      "         Iterations: 35\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2863.0            0.0\n",
      "True YES           4.0          130.0\n",
      "\n",
      "F1: 0.985\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: 0.97\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shayneufeld/anaconda/envs/cagrin/lib/python3.5/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.012396\n",
      "         Iterations: 35\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2858.0            0.0\n",
      "True YES           2.0          137.0\n",
      "\n",
      "F1: 0.993\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: 0.99\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shayneufeld/anaconda/envs/cagrin/lib/python3.5/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.011067\n",
      "         Iterations: 35\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2873.0            0.0\n",
      "True YES           3.0          121.0\n",
      "\n",
      "F1: 0.988\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: 0.98\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shayneufeld/anaconda/envs/cagrin/lib/python3.5/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.010968\n",
      "         Iterations: 35\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2854.0            0.0\n",
      "True YES           5.0          138.0\n",
      "\n",
      "F1: 0.982\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: 0.97\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shayneufeld/anaconda/envs/cagrin/lib/python3.5/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.010811\n",
      "         Iterations: 35\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2867.0            0.0\n",
      "True YES           5.0          125.0\n",
      "\n",
      "F1: 0.980\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: 0.96\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shayneufeld/anaconda/envs/cagrin/lib/python3.5/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.011053\n",
      "         Iterations: 35\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2863.0            0.0\n",
      "True YES           4.0          130.0\n",
      "\n",
      "F1: 0.985\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: 0.97\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shayneufeld/anaconda/envs/cagrin/lib/python3.5/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.011682\n",
      "         Iterations: 35\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2862.0            0.0\n",
      "True YES           3.0          132.0\n",
      "\n",
      "F1: 0.989\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: 0.98\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shayneufeld/anaconda/envs/cagrin/lib/python3.5/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.009077\n",
      "         Iterations: 35\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2846.0            3.0\n",
      "True YES           4.0          144.0\n",
      "\n",
      "F1: 0.976\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: 0.97\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shayneufeld/anaconda/envs/cagrin/lib/python3.5/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.011080\n",
      "         Iterations: 35\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2864.0            0.0\n",
      "True YES           3.0          130.0\n",
      "\n",
      "F1: 0.989\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: 0.98\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shayneufeld/anaconda/envs/cagrin/lib/python3.5/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.012422\n",
      "         Iterations: 35\n",
      "          Predicted NO  Predicted YES\n",
      "True NO         2871.0            0.0\n",
      "True YES           3.0          123.0\n",
      "\n",
      "F1: 0.988\n",
      "\n",
      "Accuracy on class 0: 1.00\n",
      "Accuracy on class 1: 0.98\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shayneufeld/anaconda/envs/cagrin/lib/python3.5/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: inf\n",
      "         Iterations: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shayneufeld/anaconda/envs/cagrin/lib/python3.5/site-packages/statsmodels/discrete/discrete_model.py:1214: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "/Users/shayneufeld/anaconda/envs/cagrin/lib/python3.5/site-packages/statsmodels/discrete/discrete_model.py:1264: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(np.log(self.cdf(q*np.dot(X,params))))\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-1d0c9015f1e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mmodel_curr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstats_curr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcoefs_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogreg_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_rewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mstats_curr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'No. parameters'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-d5ffe30cc168>\u001b[0m in \u001b[0;36mlogreg_and_eval\u001b[0;34m(data, num_rewards, test_data)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;31m#calculating pseudo-R2 and BIC from statsmodel OLS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mrslt\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m#switches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shayneufeld/anaconda/envs/cagrin/lib/python3.5/site-packages/statsmodels/discrete/discrete_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, start_params, method, maxiter, full_output, disp, callback, **kwargs)\u001b[0m\n\u001b[1;32m   1375\u001b[0m         bnryfit = super(Logit, self).fit(start_params=start_params,\n\u001b[1;32m   1376\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m                 disp=disp, callback=callback, **kwargs)\n\u001b[0m\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \u001b[0mdiscretefit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogitResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbnryfit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shayneufeld/anaconda/envs/cagrin/lib/python3.5/site-packages/statsmodels/discrete/discrete_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, start_params, method, maxiter, full_output, disp, callback, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m         mlefit = super(DiscreteModel, self).fit(start_params=start_params,\n\u001b[1;32m    203\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 disp=disp, callback=callback, **kwargs)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmlefit\u001b[0m \u001b[0;31m# up to subclasses to wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shayneufeld/anaconda/envs/cagrin/lib/python3.5/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mHinv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcov_params_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'newton'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfull_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mHinv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mretvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hessian'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip_hessian\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shayneufeld/anaconda/envs/cagrin/lib/python3.5/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m     \u001b[0mainv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shayneufeld/anaconda/envs/cagrin/lib/python3.5/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Singular matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "d = data_80.copy()\n",
    "stats = pd.DataFrame(columns=['Accuracy','BIC','negative loglikelihood','pseudo-R2','No. parameters'])\n",
    "\n",
    "for i,n in enumerate(np.arange(10,0,-1)):\n",
    "    \n",
    "    for j in enumerate(range(30)):\n",
    "        model_curr,stats_curr,coefs_curr = logreg_and_eval(d,num_rewards=n)\n",
    "        models.append(models)\n",
    "        stats_curr['No. parameters'] = n\n",
    "        stats = stats.append(stats_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x104221780>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fe31668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGUCAYAAADaqT52AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10421d780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "sns.factorplot(x='No. parameters',y='BIC',data=stats,color='black')\n",
    "plt.title('BIC vs model flexibility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BIC = np.zeros((10,10))\n",
    "F1 = np.zeros((10,10))\n",
    "R2 = np.zeros((10,10))\n",
    "acc = np.zeros((10,10))\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if not ((j==0) & (i==0)):\n",
    "            model_curr,stats_curr,coefs_curr = logreg_and_eval_withports(d,num_rewards=i,num_ports=j)\n",
    "            BIC[i,j] = stats_curr['BIC'].values\n",
    "            F1[i,j] = stats_curr['F1'].values\n",
    "            R2[i,j] = stats_curr['pseudo-R2'].values\n",
    "            acc[i,j] = stats_curr['Accuracy'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.suptitle('Model behavior vs # of rewards and decisions included',x=0.5,y=1.05,fontsize=20)\n",
    "\n",
    "plt.subplot(141)\n",
    "sns.heatmap(BIC,vmin=1500,vmax=3000)\n",
    "plt.xlabel('# previous decisions in model',fontsize=15)\n",
    "plt.ylabel('# previous reward outcomes in model',fontsize=15)\n",
    "plt.title('BIC',fontsize=15)\n",
    "\n",
    "plt.subplot(142)\n",
    "sns.heatmap(F1)\n",
    "plt.xlabel('# previous decisions in model',fontsize=15)\n",
    "plt.ylabel('# previous reward outcomes in model',fontsize=15)\n",
    "plt.title('F1',fontsize=15)\n",
    "\n",
    "plt.subplot(143)\n",
    "sns.heatmap(R2,vmin=0.5)\n",
    "plt.xlabel('# previous decisions in model',fontsize=15)\n",
    "plt.ylabel('# previous reward outcomes in model',fontsize=15)\n",
    "plt.title('R2')\n",
    "\n",
    "plt.subplot(144)\n",
    "sns.heatmap(acc,vmin=0.8)\n",
    "plt.xlabel('# previous decisions in model',fontsize=15)\n",
    "plt.ylabel('# previous reward outcomes in model',fontsize=15)\n",
    "plt.title('Accuracy')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training / testing on different conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train on 90-10, test on 80-20 and 70-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,d in enumerate([data_90,data_80,data_70]):\n",
    "    \n",
    "    if i == 0:\n",
    "        model,stats,coefs = logreg_and_eval(d)\n",
    "    else:\n",
    "        model,stats_curr,coefs_curr = logreg_and_eval(data_90,test_data = d)\n",
    "        \n",
    "        stats = stats.append(stats_curr)\n",
    "        coefs = coefs.append(coefs_curr)\n",
    "\n",
    "stats_90 = stats.drop(['BIC','negative loglikelihood','pseudo-R2'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats_90['Testing Condition'] = [90,80,70]\n",
    "stats_90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is interesting - accuracy stays pretty much the same across conditions, but F1 goes way down. And if we take a look at the confusion tables above, we can see it is because the accuracy on the switches went down (and accuracy on stays went up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train on 80-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i,d in enumerate([data_80,data_90,data_70]):\n",
    "    \n",
    "    if i == 0:\n",
    "        model,stats,coefs = logreg_and_eval(d)\n",
    "    else:\n",
    "        model,stats_curr,coefs_curr = logreg_and_eval(data_80,test_data = d)\n",
    "        \n",
    "        stats = stats.append(stats_curr)\n",
    "        coefs = coefs.append(coefs_curr)\n",
    "\n",
    "stats_80 = stats.drop(['BIC','negative loglikelihood','pseudo-R2'],axis=1)\n",
    "\n",
    "stats_80['Testing Condition'] = [80,90,70]\n",
    "stats_80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train on 70-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i,d in enumerate([data_70,data_80,data_90]):\n",
    "    \n",
    "    if i == 0:\n",
    "        model,stats,coefs = logreg_and_eval(d)\n",
    "    else:\n",
    "        model,stats_curr,coefs_curr = logreg_and_eval(data_70,test_data = d)\n",
    "        \n",
    "        stats = stats.append(stats_curr)\n",
    "        coefs = coefs.append(coefs_curr)\n",
    "\n",
    "stats_70 = stats.drop(['BIC','negative loglikelihood','pseudo-R2'],axis=1)\n",
    "\n",
    "stats_70['Testing Condition'] = [70,80,90]\n",
    "stats_70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1s = np.vstack((stats_90['F1'].values,\n",
    "               stats_80['F1'].values[[1,0,2]],\n",
    "               stats_70['F1'].values[[2,1,0]]))\n",
    "sns.heatmap(f1s)\n",
    "plt.xticks([0.5,1.5,2.5],['90','80','70'])\n",
    "plt.yticks([0.5,1.5,2.5],['70','80','90'])\n",
    "plt.ylabel('Testing Condition')\n",
    "plt.xlabel('Training Condition')\n",
    "plt.title('F1 scores when trained & tested on different conditions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if this is right, it means that the rules are different - what predicts a switch in 90-10 does not predict a switch in 80-20. But since most of the trials follow the last one, the accuracy doesn't drop very much. So it appears to be working fine, even though it is not. \n",
    "\n",
    "Can the difference be explained in the small differences in beta coefficient values? It must be ... what else is there? They seem similar enough that I'm surprised it makes such a difference. \n",
    "\n",
    "Let's go on to train and test on separate mice:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test on separate mice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's take a quick look at the mice's performances - specifically just at p(choose high P port):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_90.insert(0,'Condition',0.9)\n",
    "data_80.insert(0,'Condition',0.8)\n",
    "data_70.insert(0,'Condition',0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = data_90.append(data_80)\n",
    "all_data = all_data.append(data_70)\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.factorplot(x='Condition',y='Higher p port',hue='Mouse ID',data = all_data,legend=False,size=5,aspect=1.7)\n",
    "plt.legend(bbox_to_anchor=(1.2,1))\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('Condition',fontsize=20)\n",
    "plt.ylabel('rate higher prob port chosen',fontsize=20)\n",
    "plt.title('Average p(choose better port) for each mouse across conditions',fontsize=20,x=0.5,y=1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so based on this. Let's start with 80-20, and do a few different comparisons. \n",
    "\n",
    "1. Start by training with harry, and testing on all the others. \n",
    "2. Then try training on volde, testing on all others. \n",
    "3. Finally train on someone in the middle - like Tom or q45, and test on others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Loop through mice\n",
    "'''\n",
    "mice = np.unique(data_80['Mouse ID'].values)\n",
    "\n",
    "stats = pd.DataFrame(columns=['Accuracy','F1','Training Mouse','Testing Mouse'])\n",
    "test_mice = []\n",
    "train_mice = []\n",
    "\n",
    "for mouse_train in mice:\n",
    "\n",
    "    d_train = data_80[data_80['Mouse ID'] == mouse_train].copy()\n",
    "\n",
    "    for i,mouse_test in enumerate(mice):\n",
    "        d_test = data_80[data_80['Mouse ID'] == mouse_test].copy()\n",
    "\n",
    "        if i == 0:\n",
    "            model,stats_curr,coefs = logreg_and_eval(d_train,test_data = d_test)\n",
    "            stats_curr = stats_curr.drop(['BIC','negative loglikelihood','pseudo-R2'],axis=1)\n",
    "            stats = stats.append(stats_curr)\n",
    "        else:\n",
    "            model,stats_curr,coefs_curr = logreg_and_eval(d_train,test_data = d_test)\n",
    "            stats_curr = stats_curr.drop(['BIC','negative loglikelihood','pseudo-R2'],axis=1)\n",
    "            stats= stats.append(stats_curr)\n",
    "            coefs = coefs.append(coefs_curr)\n",
    "\n",
    "        test_mice.append(mouse_test)\n",
    "        train_mice.append(mouse_train)\n",
    "\n",
    "stats['Testing Mouse'] = test_mice\n",
    "stats['Training Mouse'] = train_mice\n",
    "acc_matrix = np.reshape(stats['Accuracy'].values,(len(mice),-1)).T\n",
    "F1_matrix = np.reshape(stats['F1'].values,(len(mice),-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "sns.heatmap(acc_matrix)\n",
    "plt.xticks(np.arange(11)+0.5,mice,rotation='vertical')\n",
    "plt.yticks(np.arange(11)+0.5,mice[::-1],rotation='horizontal')\n",
    "plt.xlabel('Training Mouse')\n",
    "plt.ylabel('Testing Mouse')\n",
    "plt.title('Accuracy')\n",
    "\n",
    "plt.subplot(122)\n",
    "sns.heatmap(F1_matrix)\n",
    "plt.xticks(np.arange(11)+0.5,mice,rotation='vertical')\n",
    "plt.yticks(np.arange(11)+0.5,mice[::-1],rotation='horizontal')\n",
    "plt.xlabel('Training Mouse')\n",
    "plt.ylabel('Testing Mouse')\n",
    "plt.title('F1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_80.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u_switch = np.zeros(len(mice))\n",
    "u_acc = np.zeros(len(mice))\n",
    "\n",
    "for i,mouse in enumerate(mice):\n",
    "    u_switch[i] = data_80[data_80['Mouse ID']== mouse]['Switch'].mean()\n",
    "    u_acc[i] = data_80[data_80['Mouse ID']== mouse]['Higher p port'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors = sns.color_palette('hls',n_colors=len(mice))\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.suptitle('Higher switch rates anti-correlate with accuracy on test mice',fontsize=20,x=0.5,y=1.1)\n",
    "plt.subplot(121)\n",
    "for i,mouse in enumerate(mice):\n",
    "    plt.scatter(u_switch[i],acc_matrix[i,:].mean(),label=mouse,c=colors[i],s=50)\n",
    "#plt.legend(bbox_to_anchor=(1.3,1))\n",
    "plt.xlabel('mean switch rate')\n",
    "plt.ylabel('mean accuracy when used as test mouse')\n",
    "plt.ylim(0.75,1)\n",
    "plt.xlim(0,0.22)\n",
    "plt.title('Testing accuracy vs mean switch rate')\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "for i,mouse in enumerate(mice):\n",
    "    plt.scatter(u_switch[i],acc_matrix[:,i].mean(),label=mouse,c=colors[i],s=50)\n",
    "plt.legend(bbox_to_anchor=(1.3,1))\n",
    "plt.xlabel('mean switch rate')\n",
    "plt.ylabel('mean accuracy when used as train mouse')\n",
    "plt.ylim(0.75,1)\n",
    "plt.xlim(0,0.22)\n",
    "plt.title('Training accuracy vs mean switch rate')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = sns.color_palette('hls',n_colors=len(mice))\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.suptitle('Higher switch rates correlate with F1 score on test mice',fontsize=20,x=0.5,y=1.1)\n",
    "plt.subplot(121)\n",
    "for i,mouse in enumerate(mice):\n",
    "    plt.scatter(u_switch[i],F1_matrix[i,:].mean(),label=mouse,c=colors[i],s=50)\n",
    "#plt.legend(bbox_to_anchor=(1.3,1))\n",
    "plt.xlabel('mean switch rate')\n",
    "plt.ylabel('mean F1 when used as test mouse')\n",
    "#plt.ylim(0.75,1)\n",
    "#plt.xlim(0,0.22)\n",
    "plt.title('Testing F1 vs mean switch rate')\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "for i,mouse in enumerate(mice):\n",
    "    plt.scatter(u_switch[i],F1_matrix[:,i].mean(),label=mouse,c=colors[i],s=50)\n",
    "plt.legend(bbox_to_anchor=(1.3,1))\n",
    "plt.xlabel('mean switch rate')\n",
    "plt.ylabel('mean F1 when used as train mouse')\n",
    "#plt.ylim(0.75,1)\n",
    "#plt.xlim(0,0.22)\n",
    "plt.title('Training F1 vs mean switch rate')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = sns.color_palette('hls',n_colors=len(mice))\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.suptitle('Better behavior correlates with accuracy score on test mice',fontsize=20,x=0.5,y=1.1)\n",
    "plt.subplot(121)\n",
    "for i,mouse in enumerate(mice):\n",
    "    plt.scatter(u_acc[i],acc_matrix[i,:].mean(),label=mouse,c=colors[i],s=50)\n",
    "#plt.legend(bbox_to_anchor=(1.3,1))\n",
    "plt.xlabel('mean p(high p port)')\n",
    "plt.ylabel('mean acc when used as test mouse')\n",
    "#plt.ylim(0.75,1)\n",
    "#plt.xlim(0,0.22)\n",
    "plt.title('Testing Acc vs mean p(high p port)')\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "for i,mouse in enumerate(mice):\n",
    "    plt.scatter(u_acc[i],acc_matrix[:,i].mean(),label=mouse,c=colors[i],s=50)\n",
    "plt.legend(bbox_to_anchor=(1.3,1))\n",
    "plt.xlabel('mean p(high p port)')\n",
    "plt.ylabel('mean acc when used as train mouse')\n",
    "#plt.ylim(0.75,1)\n",
    "#plt.xlim(0,0.22)\n",
    "plt.title('Training acc vs mean p(high p port)')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = sns.color_palette('hls',n_colors=len(mice))\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.suptitle('p(high p port) vs F1 score',fontsize=20,x=0.5,y=1.1)\n",
    "plt.subplot(121)\n",
    "for i,mouse in enumerate(mice):\n",
    "    plt.scatter(u_acc[i],F1_matrix[i,:].mean(),label=mouse,c=colors[i],s=50)\n",
    "#plt.legend(bbox_to_anchor=(1.3,1))\n",
    "plt.xlabel('p(high p port)')\n",
    "plt.ylabel('mean F1 when used as test mouse')\n",
    "#plt.ylim(0.75,1)\n",
    "#plt.xlim(0,0.22)\n",
    "plt.title('Testing F1 vs mean switch rate')\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "for i,mouse in enumerate(mice):\n",
    "    plt.scatter(u_acc[i],F1_matrix[:,i].mean(),label=mouse,c=colors[i],s=50)\n",
    "plt.legend(bbox_to_anchor=(1.3,1))\n",
    "plt.xlabel('p(high p port)')\n",
    "plt.ylabel('mean F1 when used as train mouse')\n",
    "#plt.ylim(0.75,1)\n",
    "#plt.xlim(0,0.22)\n",
    "plt.title('Training F1 vs mean switch rate')\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cagrin]",
   "language": "python",
   "name": "conda-env-cagrin-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
